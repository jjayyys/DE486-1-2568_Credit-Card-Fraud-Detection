from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import sqlalchemy
from sqlalchemy import text
import os
import matplotlib
# à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² Backend à¹€à¸›à¹‡à¸™ Agg à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¸£à¸±à¸™à¸à¸£à¸²à¸Ÿà¹ƒà¸™ Docker (Headless) à¹„à¸”à¹‰à¹‚à¸”à¸¢à¹„à¸¡à¹ˆ Error
matplotlib.use('Agg') 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

# --- 1. Configuration (à¸›à¸£à¸±à¸šà¹€à¸žà¸·à¹ˆà¸­ Docker) ---

# à¹ƒà¸Šà¹‰à¸Šà¸·à¹ˆà¸­ Service 'mysql_db' à¹à¸—à¸™ localhost à¹€à¸žà¸£à¸²à¸° Container à¸„à¸¸à¸¢à¸à¸±à¸™à¹€à¸­à¸‡à¸œà¹ˆà¸²à¸™ Docker Network
# à¸£à¸¹à¸›à¹à¸šà¸š: mysql+pymysql://user:password@service_name:port/db_name
DB_CONNECTION_STR = 'mysql+pymysql://root:root@mysql_db:3306/transaction_db'

# Path à¸™à¸µà¹‰à¸•à¹‰à¸­à¸‡à¸•à¸£à¸‡à¸à¸±à¸šà¸—à¸µà¹ˆ Mount volume à¹„à¸§à¹‰à¹ƒà¸™ docker-compose.yaml
# à¹€à¸£à¸² Mount ./data à¹„à¸§à¹‰à¸—à¸µà¹ˆ /opt/airflow/data
DATA_PATH = '/opt/airflow/data'
CSV_FILE = os.path.join(DATA_PATH, 'transaction.csv')
VIZ_FILE = os.path.join(DATA_PATH, 'data_comparison.png')

# --- 2. ELT Functions ---

def extract_and_load_raw():
    """
    Step 1: Extract & Load
    à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œ CSV à¹à¸¥à¸°à¹‚à¸«à¸¥à¸”à¸¥à¸‡ MySQL à¸•à¸²à¸£à¸²à¸‡ 'raw_transactions' à¸—à¸±à¸™à¸—à¸µ
    """
    print(f"ðŸš€ Starting Step 1: Extract & Load")
    
    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸¡à¸µà¹„à¸Ÿà¸¥à¹Œà¸­à¸¢à¸¹à¹ˆà¸ˆà¸£à¸´à¸‡à¹„à¸«à¸¡
    if not os.path.exists(CSV_FILE):
        raise FileNotFoundError(f"âŒ à¹„à¸¡à¹ˆà¸žà¸šà¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆ: {CSV_FILE} à¸à¸£à¸¸à¸“à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸§à¸²à¸‡à¹„à¸Ÿà¸¥à¹Œà¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ data/ à¸«à¸£à¸·à¸­à¸¢à¸±à¸‡")

    # à¸­à¹ˆà¸²à¸™ CSV
    print(f"Reading CSV from {CSV_FILE}...")
    df = pd.read_csv(CSV_FILE)
    print(f"âœ… Read successfully. Raw Shape: {df.shape}")

    # à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­ Database à¹à¸¥à¸°à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
    try:
        engine = sqlalchemy.create_engine(DB_CONNECTION_STR)
        with engine.connect() as conn:
            # à¸—à¸”à¸ªà¸­à¸šà¸à¸²à¸£à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­
            print("Connected to Database successfully.")
            
        # à¹€à¸‚à¸µà¸¢à¸™à¸¥à¸‡ SQL (chunksize à¸Šà¹ˆà¸§à¸¢à¸¥à¸”à¸à¸²à¸£à¹ƒà¸Šà¹‰ Memory)
        print("Uploading to MySQL table 'raw_transactions'...")
        df.to_sql('raw_transactions', engine, if_exists='replace', index=False, chunksize=5000)
        print("âœ… Data loaded to 'raw_transactions' successfully.")
        
    except Exception as e:
        print(f"âŒ Database Error: {e}")
        raise e

def transform_in_db():
    """
    Step 2: Transform
    à¸­à¹ˆà¸²à¸™à¸ˆà¸²à¸ DB -> Clean/Feature Eng -> à¹à¸¢à¸à¸•à¸²à¸£à¸²à¸‡ -> Save à¸à¸¥à¸±à¸šà¸¥à¸‡ DB
    """
    print("ðŸš€ Starting Step 2: Transform")
    engine = sqlalchemy.create_engine(DB_CONNECTION_STR)

    # 2.1 à¸­à¹ˆà¸²à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸”à¸´à¸š
    query = "SELECT * FROM raw_transactions"
    df = pd.read_sql(query, engine)
    print(f"Fetched {len(df)} rows from DB.")
    
    # Clean Column Names
    df.columns = df.columns.str.strip()

    # --- Feature Engineering ---
    # à¸¥à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‹à¹‰à¸³
    original_len = len(df)
    df.drop_duplicates(inplace=True)
    print(f"Removed {original_len - len(df)} duplicate rows.")

    # Time Engineering
    df["Time"] = df["Time"].astype(int)
    df["day"] = df["Time"] // (3600 * 24)
    df["hour"] = (df["Time"] // 3600) % 24

    # Scaling Amount
    scaler = StandardScaler()
    df['Amount'] = df['Amount'].astype(float)
    df['Amount_Scaled'] = scaler.fit_transform(df[['Amount']])

    # à¸ªà¸£à¹‰à¸²à¸‡ ID
    df['transaction_id'] = range(1, len(df) + 1)

    # --- Splitting Tables (Normalization) ---
    cols_meta = ['transaction_id', 'Time', 'day', 'hour', 'Amount', 'Amount_Scaled', 'Class']
    v_columns = [f'V{i}' for i in range(1, 29)]
    cols_features = ['transaction_id'] + v_columns

    df_transactions = df[cols_meta]
    df_features = df[cols_features]

    # 2.2 Save à¸à¸¥à¸±à¸šà¸¥à¸‡ DB
    print("Saving processed tables...")
    df_transactions.to_sql('transactions_processed', engine, if_exists='replace', index=False, chunksize=5000)
    df_features.to_sql('transaction_features', engine, if_exists='replace', index=False, chunksize=5000)
    
    print("âœ… Transformation Completed.")

def visualize_data():
    """
    Step 3: Visualization
    à¸ªà¸£à¹‰à¸²à¸‡à¸à¸£à¸²à¸Ÿà¹à¸¥à¸°à¸šà¸±à¸™à¸—à¸¶à¸à¹€à¸›à¹‡à¸™à¹„à¸Ÿà¸¥à¹Œ PNG à¸à¸¥à¸±à¸šà¹„à¸›à¸—à¸µà¹ˆà¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ data
    """
    print("ðŸš€ Starting Step 3: Visualization")
    engine = sqlalchemy.create_engine(DB_CONNECTION_STR)

    # à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸³à¸™à¸§à¸™à¹à¸–à¸§à¸¡à¸²à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š
    with engine.connect() as conn:
        raw_count = conn.execute(text("SELECT COUNT(*) FROM raw_transactions")).scalar()
        processed_count = conn.execute(text("SELECT COUNT(*) FROM transactions_processed")).scalar()

    # à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸žà¸·à¹ˆà¸­à¸žà¸¥à¸­à¸•à¸à¸£à¸²à¸Ÿ Fraud
    df_clean = pd.read_sql("SELECT hour, Class FROM transactions_processed", engine)

    # à¹€à¸£à¸´à¹ˆà¸¡à¸§à¸²à¸”à¸à¸£à¸²à¸Ÿ
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # à¸à¸£à¸²à¸Ÿ 1: Data Loss
    axes[0].bar(['Raw Data', 'Cleaned Data'], [raw_count, processed_count], color=['gray', 'green'])
    axes[0].set_title(f'Data Count Comparison\n(Lost {raw_count - processed_count} duplicates)')
    axes[0].set_ylabel('Number of Rows')

    # à¸à¸£à¸²à¸Ÿ 2: Fraud Pattern
    fraud_data = df_clean[df_clean['Class'] == 1]
    if not fraud_data.empty:
        sns.histplot(data=fraud_data, x='hour', bins=24, color='red', kde=True, ax=axes[1])
        axes[1].set_title('Fraud Transactions by Hour (Cleaned Data)')
        axes[1].set_xlabel('Hour of Day')
    else:
        axes[1].text(0.5, 0.5, 'No Fraud Data Found', ha='center')

    plt.tight_layout()
    
    # à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸Ÿà¸¥à¹Œ
    plt.savefig(VIZ_FILE)
    print(f"âœ… Visualization saved at: {VIZ_FILE}")

# --- 3. DAG Definition ---

default_args = {
    'owner': 'somprat',  # à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¸Šà¸·à¹ˆà¸­à¹€à¸ˆà¹‰à¸²à¸‚à¸­à¸‡à¹„à¸”à¹‰à¸•à¸²à¸¡à¸•à¹‰à¸­à¸‡à¸à¸²à¸£
    'depends_on_past': False,
    'email_on_failure': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id='fraud_detection_docker_pipeline',  # à¸Šà¸·à¹ˆà¸­à¸—à¸µà¹ˆà¸ˆà¸°à¸‚à¸¶à¹‰à¸™à¹ƒà¸™à¸«à¸™à¹‰à¸²à¹€à¸§à¹‡à¸š Airflow
    default_args=default_args,
    description='ELT Pipeline for Fraud Detection on Docker',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily',
    catchup=False,
    tags=['docker', 'fraud-detection'],
) as dag:

    t1_load_raw = PythonOperator(
        task_id='1_extract_and_load_raw',
        python_callable=extract_and_load_raw
    )

    t2_transform = PythonOperator(
        task_id='2_transform_in_db',
        python_callable=transform_in_db
    )

    t3_visualize = PythonOperator(
        task_id='3_generate_visualization',
        python_callable=visualize_data
    )

    # à¸à¸³à¸«à¸™à¸”à¸¥à¸³à¸”à¸±à¸šà¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™
    t1_load_raw >> t2_transform >> t3_visualize
